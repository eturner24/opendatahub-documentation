:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

:context: serving-large-models

[id="serving-large-models_{context}"]
= Serving large models

[role='_abstract']
For deploying large models such as large language models (LLMs), {productname-long} includes a _single-model serving platform_ that is based on the KServe component. Because each model is deployed from its own model server, the single-model serving platform helps you to deploy, monitor, scale, and maintain large models that require increased resources.

== Deploying and serving large models

include::modules/deploying-models-using-the-single-model-serving-platform.adoc[leveloffset=+1]
include::modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform.adoc[leveloffset=+2]

== Monitoring model peformance
// Conditionalized for self-managed because monitoring of user-defined projects is enabled on OSD and ROSA by default
ifdef::upstream,self-managed[]
include::modules/configuring-monitoring-for-the-single-model-serving-platform.adoc[leveloffset=+1]
endif::[]

include::modules/viewing-metrics-for-the-single-model-serving-platform.adoc[leveloffset=+1]

== Improving performance on the single-model serving platform
Certain performance issues might require you to tune the parameters of your inference service or model-serving runtime.

include::modules/resolving-cuda-oom-errors.adoc[leveloffset=+2]

// [role='_additional-resources']
// == Additional resources

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
