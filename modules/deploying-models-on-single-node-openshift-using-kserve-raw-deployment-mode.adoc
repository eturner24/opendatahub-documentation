:_module-type: PROCEDURE

[id="deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode_{context}"]
= Preparing your cluster for standard mode deployment

[role='_abstract']

You can deploy a machine learning model by using standard deployment mode, which is based on KServe RawDeployment mode on {openshift-platform}. Standard mode offers several advantages over advanced, such as the ability to mount multiple volumes. 

.Prerequisites
* You have logged in to {productname-long}.
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have created an {openshift-platform} cluster that has a node with at least 4 CPUs and 16 GB memory.
* You have installed the {productname-long} (RHOAI) Operator.
* You have installed the OpenShift command-line interface (CLI). For more information about installing the OpenShift command-line interface (CLI), see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#cli-getting-started[Getting started with the OpenShift CLI].
* You have installed KServe.
//* You have enabled the single-model serving platform.
* You have access to S3-compatible object storage.
* For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.
* To use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an example, see link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process[Converting Hugging Face Hub models to Caikit format^] in the link:https://github.com/opendatahub-io/caikit-tgis-serving/tree/main[caikit-tgis-serving^] repository.
ifndef::upstream[]
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. If you use NVIDIA GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^]. If you use AMD GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#amd-gpu-integration_managing-rhoai[AMD GPU integration^].
* To use the vLLM runtime, you have enabled GPU support in {productname-short} and have installed and configured the Node Feature Discovery operator on your cluster. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs].
endif::[]
ifdef::upstream[]
* To use the vLLM runtime or use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]

.Procedure
. Open a command-line terminal and log in to your {openshift-platform} cluster as cluster administrator:
+
[source]
----
$ oc login <openshift_cluster_url> -u <admin_username> -p <password>
----

. By default, {openshift-platform} uses a service mesh for network traffic management. Because KServe raw deployment mode does not require a service mesh, disable {org-name} {openshift-platform} Service Mesh:
.. Enter the following command to disable {org-name} {openshift-platform} Service Mesh:
+
[source]
----
$ oc edit dsci -n redhat-ods-operator
----
.. In the YAML editor, change the value of `managementState` for the `serviceMesh` component to `Removed` as shown:
+
[source]
----
spec:
  components:
    serviceMesh:
      managementState: Removed
----
.. Save the changes.
. Create a project:
+
[source]
----
$ oc new-project <project_name> --description="<description>" --display-name="<display_name>"
----
For information about creating projects, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/building_applications/projects#working-with-projects[Working with projects].

. Create a data science cluster:
.. In the {org-name} {openshift-platform} web console *Administrator* view, click *Operators* â†’ *Installed Operators* and then click the {productname-long} Operator.
.. Click the *Data Science Cluster* tab.
.. Click the *Create DataScienceCluster* button.
.. In the *Configure via* field, click the *YAML view* radio button.
.. In the `spec.components` section of the YAML editor, configure the `kserve` component as shown:
+
[source]
----
  kserve:
    defaultDeploymentMode: RawDeployment
    managementState: Managed
    serving:
      managementState: Removed
      name: knative-serving
----
.. Click *Create*.
+
. Create a secret file: 
.. At your command-line terminal, create a YAML file to contain your secret and add the following YAML code:
+
[source]
----
apiVersion: v1
kind: Secret
metadata:
  annotations:
    serving.kserve.io/s3-endpoint: <AWS_ENDPOINT>
    serving.kserve.io/s3-usehttps: "1"
    serving.kserve.io/s3-region: <AWS_REGION>
    serving.kserve.io/s3-useanoncredential: "false"
  name: <Secret-name>
stringData:
  AWS_ACCESS_KEY_ID: "<AWS_ACCESS_KEY_ID>"
  AWS_SECRET_ACCESS_KEY: "<AWS_SECRET_ACCESS_KEY>"
----
+
[IMPORTANT]
====
If you are deploying a machine learning model in a disconnected deployment, add `serving.kserve.io/s3-verifyssl: '0'` to the `metadata.annotations` section.
====
.. Save the file with the file name *secret.yaml*.
.. Apply the *secret.yaml* file:
+
[source]
----
$ oc apply -f secret.yaml -n <namespace>
----

// . Verification
// How do they verify that everything is set up correctly?