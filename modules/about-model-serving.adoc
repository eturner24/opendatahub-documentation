:_module-type: CONCEPT

[id="about-model-serving_{context}"]
= About model serving

[role="_abstract"]
When you serve a model, you upload a trained model into {productname-long} for querying, which allows you to integrate your trained models into intelligent applications.

First, you upload the model to an S3-compatible storage container, persistent volume claim, or Open Container Initiative (OCI) image. Then, you serve trained models on your {openshift-platform} cluster. Serving or deploying models makes the model available as a service, or model runtime server, that you can access using an API. 

After you serve a model, you can access inference endpoints for the deployed model from the dashboard. You can see predictions based on data inputs that you provide through API calls. Querying the model through the API is also called model inferencing. 

You can serve models on one of the following model-serving platforms:
* Single-model serving platform
* Multi-model serving platform
* NVIDIA NIM-model serving platform

The model-serving platform that you choose depends on your business needs:
* If you want to deploy each model on its own runtime server, or want to use a serverless deployment, select the *single-model serving platform*. The single-model serving platform is recommended for production use.
* If you want to deploy multiple models with only one runtime server, select the *multi-model serving platform*. This option is best if you are deploying more than 1,000 models and want to reduce resource consumption.
* If you are using the NVIDIA serving runtime and NVIDIA NIMs, select the *NVIDIA NIM-model serving platform*.

Single-model serving platform::
You can deploy each model from a dedicated model serving on the single-model serving platform. Deploying models from a dedicated model server can help you deploy, monitor, scale, and maintain models that require increased resources. The single-model serving platform is based on the link:https://github.com/kserve/kserve[KServe^] component.

The single-model serving platform is helpful for use cases such as:
* Large language models (LLMs)
* Generative AI

ifndef::upstream[]
For more information about setting up the single-model serving platform, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed/installing-the-single-model-serving-platform_component-install[Installing the single-model serving platform].
endif::[]

Multi-model serving platform::
You can deploy multiple models from the same model server on the multi-model serving platform. Each of the deployed models shares the server resources. Deploying multiple models from the same model server can be advantageous on OpenShift clusters that have finite compute resources or pods. The multi-model serving platform is based on the link:https://github.com/kserve/modelmesh[ModelMesh^] component. 

ifndef::upstream[]
For more information about setting up the multi-model serving platform, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed/installing-the-multi-model-serving-platform_component-install[Installing the multi-model serving platform].
endif::[]

NVIDIA NIM model serving platform::

You can deploy models using NVIDIA NIM inference services on the NVIDIA NIM model serving platform.

NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.

NVIDIA NIM inference services are helpful for use cases such as:
* Using GPU-accelerated containers inferencing models optimized by NVIDIA
* Deploying generative AI for virtual screening, content generation, and avatar creation

ifndef::upstream[]
The NVIDIA NIM model serving platform is based on the single-model serving platform. To use the NVIDIA NIM model serving platform, you must first install the single-model serving platform. For more information, see see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed/installing-the-single-model-serving-platform_component-install[Installing the single-model serving platform].
endif::[]

// [role="_additional-resources"]
// .Additional resources