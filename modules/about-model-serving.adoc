:_module-type: CONCEPT

[id="about-model-serving_{context}"]
= About model serving

[role="_abstract"]
Serving trained models on {productname-long} means deploying the models on your OpenShift cluster to test and then integrate them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API. This enables you to return predictions based on data inputs that you provide through API calls. This process is known as model _inferencing_. When you serve a model on {productname-short}, the inference endpoints that you can access for the deployed model are shown in the dashboard. 

// [role="_additional-resources"]
// .Additional resources
