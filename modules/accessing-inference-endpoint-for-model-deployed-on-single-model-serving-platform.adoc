:_module-type: PROCEDURE

[id="accessing-inference-endpoint-for-deployed-model_{context}"]
= Accessing the inference endpoint for a deployed model

[role='_abstract']
To make inference requests to your deployed model, you must know how to access the inference endpoint that is available.

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have deployed a model by using the single-model serving platform.
* If you enabled token authorization for your deployed model, you have the associated token value.

.Procedure
. From the {productname-short} dashboard, click *Model Serving*.
+
The inference endpoint for the model is shown in the *Inference endpoint* field.
. Depending on what action you want to perform with the model (and if the model supports that action), copy the inference endpoint shown and then add one of the following paths to the end of the URL:
+
--
*Caikit TGIS ServingRuntime for KServe*

* `:443/api/v1/task/text-generation`
* `:443/api/v1/task/server-streaming-text-generation`
// * `:443/api/v1/task/text-classification`
// * `:443/api/v1/task/token-classification`

*Caikit Standalone ServingRuntime*

By default, the Caikit Standalone ServingRuntime exposes REST endpoints for use. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime.

.HTTP endpoints
* `/api/v1/task/embedding`
* `/api/v1/task/embedding-tasks`
* `/api/v1/task/sentence-similarity`
* `/api/v1/task/sentence-similarity-tasks`
* `/api/v1/task/rerank`
* `/api/v1/task/rerank-tasks`

.gRPC endpoints
* `:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/EmbeddingTasksPredict`
* `:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTasksPredict`
* `:443 caikit.runtime.Nlp.NlpService/RerankTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/RerankTasksPredict`

*TGIS Standalone ServingRuntime for KServe*

* `:443 fmaas.GenerationService/Generate`
* `:443 fmaas.GenerationService/GenerateStream`
+
NOTE: To query the endpoint for the TGIS standalone runtime, you must also download the files in the link:https://github.com/opendatahub-io/text-generation-inference/blob/main/proto[proto^] directory of the Open Data Hub `text-generation-inference` repository.

*OpenVINO Model Server*

* `/v2/models/<model-name>/infer`

*vLLM ServingRuntime for KServe*

* `:443/version`
* `:443/docs`
* `:443/v1/models`
* `:443/v1/chat/completions`
* `:443/v1/completions`
* `:443/v1/embeddings`
+
NOTE: The vLLM runtime is compatible with the OpenAI REST API. For a list of models that the vLLM runtime supports, see link:https://docs.vllm.ai/en/latest/models/supported_models.html[Supported models].
+
NOTE: To use the embeddings inference endpoint in vLLM, you must use an embeddings model that the vLLM supports. You cannot use the embeddings endpoint with generative models. For more information, see link:https://github.com/vllm-project/vllm/pull/3734[Supported embeddings models in vLLM].
+

As indicated by the paths shown, the single-model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.
--

. Use the endpoint to make API requests to your deployed model, as shown in the following example commands.

ifdef::upstream[]
+
--
*Caikit TGIS ServingRuntime for KServe*
[source,subs="+quotes"]
----
curl --json '{"model_id": "<model_name>", "inputs": "<text>"}' \
https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation \
-H 'Authorization: Bearer <token>' <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.

*Caikit Standalone ServingRuntime for KServe*

.HTTP
[source]
----
curl -H 'Content-Type: application/json' -d '{"inputs": "<text>", "model_id": "<model_id>"}' /api/v1/task/embedding
----

.gRPC
[source]
----
grpcurl -insecure -d '{"text": "<text>"}' -H \"mm-model-id: <model_id>\" <inference_endpoint_url>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict
----

*TGIS Standalone ServingRuntime for KServe*
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d \
'{"requests": [{"text":"<text>"}]}' \
-insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate \
-H 'Authorization: Bearer <token>' <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.

*OpenVINO Model Server*
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d \
'{ "model_name": "<model_name>", \
"inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' \
-H 'Authorization: Bearer <token>' <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.

*vLLM ServingRuntime for KServe*
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H \
"Content-Type: application/json" -d '{ \
"messages": [{ \
"role": "<role>", \
"content": "<content>" \
}] -H 'Authorization: Bearer <token>' <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.
--
endif::[]
ifdef::self-managed,cloud-service[]

+
--
*Caikit TGIS ServingRuntime for KServe*
[source]
----
curl --json '{"model_id": "<model_name__>", "inputs": "<text>"}' https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation -H 'Authorization: Bearer <token>'  <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.

*Caikit Standalone ServingRuntime for KServe*
REST
[source]
----
curl -H 'Content-Type: application/json' -d '{"inputs": "<text>", "model_id": "<model_id>"}' <inference_endpoint_url>/api/v1/task/embedding
----

gRPC
[source]
----
grpcurl -insecure -d '{"text": "<text>"}' -H \"mm-model-id: <model_id>\" <inference_endpoint_url>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict
----

*TGIS Standalone ServingRuntime for KServe*
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d '{"requests": [{"text":"<text>"}]}' -H 'Authorization: Bearer <token>' -insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate  <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.

*OpenVINO Model Server*
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'  <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.

*vLLM ServingRuntime for KServe*
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H "Content-Type: application/json" -d '{ "messages": [{ "role": "<role>", "content": "<content>" }] -H 'Authorization: Bearer <token>' <1>
----
<1> Add the `Authorization` header and specify a token value only if you enabled token authorization when deploying the model.
--
endif::[]

[role='_additional-resources']
.Additional resources
* link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^]
* link:https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html[Caikit API documentation^]
* link:https://github.com/markstur/caikit-embeddings[Caikit Text Embedding GitHub project^]
* link:https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html[OpenVINO KServe-compatible REST API documentation^]
* link:https://platform.openai.com/docs/api-reference/introduction[OpenAI API documentation]
